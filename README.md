# MNIST Classification with PyTorch

![Tests](https://github.com/twitu/mnist-solver-pro-max/actions/workflows/model_tests.yml/badge.svg)

## Overview
This project implements a Convolutional Neural Network (CNN) for MNIST digit classification using PyTorch. The model achieves >99.4% validation accuracy while maintaining a lightweight architecture (<20K parameters) through efficient use of modern deep learning techniques.

## Model Architecture
The network architecture incorporates several key features:
- Three sequential blocks with convolutional layers
- Batch Normalization for training stability
- Dropout (0.1) for regularization
- Global Average Pooling to reduce parameters
- 1x1 convolutions for channel dimension reduction

## Key Features
- Parameters: <20K
- Training Duration: 20 epochs
- Validation Accuracy: >99.4%
- Data Augmentation: Random rotation and affine transformations
- Learning Rate Scheduling: OneCycleLR policy

## Requirements
python
torch
torchvision
tqdm
matplotlib
torchsummary

## Usage
python
python script.py

## Model Performance
- The model achieves the target accuracy within the specified constraints
- Uses modern architectural choices to maintain efficiency
- Implements data augmentation for improved generalization

## Training Details
- Optimizer: Adam with weight decay
- Learning Rate: OneCycleLR scheduling
- Batch Size: 32
- Data Augmentation: 
  - Random rotation (±5 degrees)
  - Random affine transformation with shear

## Test logs

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 28, 28]             100
       BatchNorm2d-2           [-1, 10, 28, 28]              20
            Conv2d-3           [-1, 20, 28, 28]           1,820
       BatchNorm2d-4           [-1, 20, 28, 28]              40
         MaxPool2d-5           [-1, 20, 14, 14]               0
           Dropout-6           [-1, 20, 14, 14]               0
            Conv2d-7           [-1, 10, 14, 14]             210
            Conv2d-8           [-1, 20, 14, 14]           1,820
       BatchNorm2d-9           [-1, 20, 14, 14]              40
           Conv2d-10           [-1, 20, 14, 14]           3,620
      BatchNorm2d-11           [-1, 20, 14, 14]              40
        MaxPool2d-12             [-1, 20, 7, 7]               0
          Dropout-13             [-1, 20, 7, 7]               0
           Conv2d-14             [-1, 40, 7, 7]           7,240
      BatchNorm2d-15             [-1, 40, 7, 7]              80
AdaptiveAvgPool2d-16             [-1, 40, 1, 1]               0
           Conv2d-17             [-1, 10, 1, 1]             410
================================================================
Total params: 15,440
Trainable params: 15,440
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.60
Params size (MB): 0.06
Estimated Total Size (MB): 0.66
----------------------------------------------------------------
loss=0.1831241399049759 batch_id=1874: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:25<00:00, 72.15it/s]

Test set: Average loss: 0.1796, Accuracy: 9454/10000 (95%)

loss=0.1384226381778717 batch_id=1874: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:25<00:00, 72.45it/s]

Test set: Average loss: 0.0730, Accuracy: 9753/10000 (98%)

loss=0.007305058650672436 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:25<00:00, 72.34it/s]

Test set: Average loss: 0.0482, Accuracy: 9834/10000 (98%)

loss=0.023853518068790436 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 69.85it/s]

Test set: Average loss: 0.0957, Accuracy: 9680/10000 (97%)

loss=0.04571952670812607 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 71.61it/s]

Test set: Average loss: 0.0991, Accuracy: 9679/10000 (97%)

loss=0.021880673244595528 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:25<00:00, 72.33it/s]

Test set: Average loss: 0.0602, Accuracy: 9811/10000 (98%)

loss=0.02187895029783249 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 71.38it/s]

Test set: Average loss: 0.0607, Accuracy: 9797/10000 (98%)

loss=0.09781094640493393 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:27<00:00, 67.91it/s]

Test set: Average loss: 0.0393, Accuracy: 9867/10000 (99%)

loss=0.17770624160766602 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:27<00:00, 69.34it/s]

Test set: Average loss: 0.0505, Accuracy: 9823/10000 (98%)

loss=0.05413908138871193 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:27<00:00, 69.43it/s]

Test set: Average loss: 0.0448, Accuracy: 9855/10000 (99%)

loss=0.006068249698728323 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 69.97it/s]

Test set: Average loss: 0.0378, Accuracy: 9875/10000 (99%)

loss=0.05170372501015663 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 71.58it/s]

Test set: Average loss: 0.0314, Accuracy: 9906/10000 (99%)

loss=0.030427588149905205 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:27<00:00, 67.34it/s]

Test set: Average loss: 0.0278, Accuracy: 9897/10000 (99%)

loss=0.014592444524168968 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 70.64it/s]

Test set: Average loss: 0.0281, Accuracy: 9911/10000 (99%)

loss=0.012730930000543594 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 72.05it/s]

Test set: Average loss: 0.0242, Accuracy: 9914/10000 (99%)

loss=0.012026207521557808 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 71.37it/s]

Test set: Average loss: 0.0195, Accuracy: 9935/10000 (99%)

loss=0.003149487543851137 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 72.03it/s]

Test set: Average loss: 0.0170, Accuracy: 9943/10000 (99%)

loss=0.05234778672456741 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 71.40it/s]

Test set: Average loss: 0.0162, Accuracy: 9954/10000 (100%)

loss=0.04681713879108429 batch_id=1874: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 72.06it/s]

Test set: Average loss: 0.0160, Accuracy: 9948/10000 (99%)

loss=0.032981500029563904 batch_id=1874: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:26<00:00, 71.66it/s]

Test set: Average loss: 0.0153, Accuracy: 9952/10000 (100%)
```
